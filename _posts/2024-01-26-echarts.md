---
layout: post
title: recent thoughts on unsupervised learning
date: 2024-01-26 16:03:00
description: this is what included echarts code could look like
tags: formatting charts
categories: sample-posts
chart:
  echarts: true
---

从最近的GTC上 Vincent Vanhoucke talk 到Ilya Sutskever 的 "an observation on generalization"，到"Video as the New Language for Real-World Decision Making", 再到最近有关自动驾驶思考中的一些困惑。

第一点

google deepmind 做robotics的似乎分成了两个流派： 
text as general interface: 一个是以Vincent Vanhoucke 带领的用llm/vlm (ar model) 的流派，坚定autoregressive + scaling 形式，everything (code, action, etc.) is just another form of text, 从high level 到 low level 逐渐更深层次替换成lm, 并且claim 该方法目前并没有看到上限 （但是困惑在于，目前在google brain 的robotics 工作中环境基本上都是偏静态的，像自动驾驶这样强交互的场景, is llm enough?）

video as general interface: 另一个是以sherry yang， yilun du (还不确定这个流派背后大佬是谁) ，从visual/physical world 本身出发，探索以video 作为一种新的语言，以video 作为decision making 的基础，积极探索以视频生成的方式做planning/decision making/simulation。视频生成的方式大部分的探索是在diffusion 方向, 但因为video generation 的模型还没有像nlp 领域一样converge, 所以diffusion model/ar model/masked model 都是现阶段实现的可能方式之一

在 Video as the New Language for Real-World Decision Making 这篇文章中，作者们identify 了3个语言模型大一统的key components: 1. a unified representation (text), 2. a unified interface (text generation), 3. language model's ability to interact with external environments by taking actions and optimizing decisions based on external feekback. 

autoregressive + scaling law 基本上是现阶段验证的唯一work 的framework 了，所以不难理解在不同的领域都在尝试把把自己的domain 的信息tokenize，然后放在这个大一统框架下。以上这些叙述感觉都是在把ar 看成是一个conditional distribution 来看。
<!-- 但是困惑在于，目前在google brain 的robotics 工作中环境基本上都是静态的，像自动驾驶 -->

第二点是an observation on generalization，一个很好的总结在https://sumanthrh.com/post/notes-on-generalization/

和上面的叙述不同，这里ar 主要是一个从compression 的角度的叙述。这样看
chatgpt 因为统一了representation 和interface, unsupervised learning (pretrain stage) 能帮助supervised finetuning 获得更好的compression (compression is prediction)
（lecake 的三层蛋糕）

unsupervised learning 的目的是exploit 数据间的algorithmic mutual information。y is the data of your supervised task, x is the data of your unsupervised task
you optimize one objective but you care about another objective. 
one to one correspondence between compression and prediction 
formalize it:
low regret = "we got all the value" out of the unlablled data x
kolmogorov complexity as the ultimate compressor, gives the ultimate low regret algorithm
kolmogorov comlexity: k(x) the length of the shortest program that outputs x
joint compression is maximum likelihood (if we don't overfit)
the sum of the negative log likelihood on the datasets is the cost of compression the dataset
big nns approximate kolmogorov compressor better and better

从kolmogorov comlexity 角度来说，任何能产生概率的模型都能放在这个理论框架下，为什么选用autoregressive model, 因为表达形式简单。理论上diffusion model （maximize prob 的那一个formulation）, energy based model 也可以适用。

how about in autonomous driving? 大家都说的foundation model 是什么？这个和我们实际下游要做的task 是什么关系？在上述的这个角度看来，foundation model 的pretraining 要做的就是能帮助在下游任务上获得更好的compressor。 根据上面的理论，foundation model 选择要是能够产生概率分布的，最好模型结构要简单通用，有扩展性，那么ar model 可能是目前最符合这个条件的模型。


虽然第二点这个感觉也很自洽，但是is this all need for decision making (一个foundation model + xx head)? 或者说world model learn 了compressor，然后在基于此之上做decision making (dream 系列或者 更偏向于 Yann LeCun autonomous agent 设想)，抑或是像video as a new language 一样有一个新的interface 和representation 之后，整个decision making 就大一统在这个框架下了？

同时对于driving这个task来说，最终需要gpt 这样的学习范式才能解决吗？



